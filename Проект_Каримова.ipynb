{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hineri-san/project_Karimova/blob/main/%D0%9F%D1%80%D0%BE%D0%B5%D0%BA%D1%82_%D0%9A%D0%B0%D1%80%D0%B8%D0%BC%D0%BE%D0%B2%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ7m6FDNrE5s"
      },
      "outputs": [],
      "source": [
        "!pip install pymorphy2\n",
        "!pip install stanza\n",
        "\n",
        "import stanza\n",
        "stanza.download('ru')\n",
        "nlp = stanza.Pipeline('ru', processors='tokenize,lemma')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/named-entity/hse-nlp/raw/master/4th_year/Project/train_reviews.txt\n",
        "!wget https://github.com/named-entity/hse-nlp/raw/master/4th_year/Project/train_aspects.txt\n",
        "!wget https://github.com/named-entity/hse-nlp/raw/master/4th_year/Project/dev_reviews.txt"
      ],
      "metadata": {
        "id": "3la-0nYfroMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_aspects = pd.read_csv('train_aspects.txt',\n",
        "                             delimiter='\\t', \n",
        "                             names=['text_id', 'category', 'mention', 'start', 'end', 'sentiment'])"
      ],
      "metadata": {
        "id": "77vwPjcYsvmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_aspects.head()"
      ],
      "metadata": {
        "id": "If1yV6-_s2Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = pd.read_csv('train_reviews.txt', \n",
        "                          delimiter='\\t', \n",
        "                          names=['text_id','text'])"
      ],
      "metadata": {
        "id": "RhmKa_zGtdkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts.head()"
      ],
      "metadata": {
        "id": "JxkaKat5tx9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_texts = pd.read_csv('dev_reviews.txt', \n",
        "                          delimiter='\\t', \n",
        "                          names=['text_id','text'])"
      ],
      "metadata": {
        "id": "DuJ65LrITw0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 1 и 2 | Baseline"
      ],
      "metadata": {
        "id": "hJbkYgbO-KJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Экспериментировать будем на основе бейзлайна, поэтому сразу подгружаем оттуда все необходимые функции"
      ],
      "metadata": {
        "id": "S5i3izQmWn8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    doc = nlp(text)\n",
        "    words = [word.lemma for sent in doc.sentences for word in sent.words]\n",
        "    return words\n",
        "\n",
        "train_aspects['norm_mention'] = [tuple(normalize(m)) for m in train_aspects['mention']]"
      ],
      "metadata": {
        "id": "6T1h4NBF7Ejp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mention_category(data, cat_type):\n",
        "    mention_categories = data.value_counts(subset=['norm_mention', cat_type])\n",
        "    mention_categories_dict = defaultdict(dict)\n",
        "    for key, value in mention_categories.items():\n",
        "        mention_categories_dict[key[0]][key[1]] = value\n",
        "    return {k: Counter(v).most_common(1)[0][0] for k, v in mention_categories_dict.items()}\n",
        "    \n",
        "best_mention_cat = get_mention_category(train_aspects, 'category')\n",
        "best_mention_sentiment = get_mention_category(train_aspects, 'sentiment')\n"
      ],
      "metadata": {
        "id": "RKg40uXZ89Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_texts(text, mentions, sentiments, max_len=5):\n",
        "    tokenized = [word for sent in nlp(text).sentences for word in sent.words]\n",
        "    text_end = len(tokenized)\n",
        "    for i, token in enumerate(tokenized):\n",
        "        for l in reversed(range(max_len)):\n",
        "            if i + l > text_end:\n",
        "                continue\n",
        "            span = tokenized[i:i + l]\n",
        "            key = tuple([t.lemma for t in span])\n",
        "            if key in mentions:\n",
        "                start, end = span[0].start_char, span[-1].end_char\n",
        "                yield mentions[key], text[start:end], start, end, sentiments[key]\n",
        "                break"
      ],
      "metadata": {
        "id": "avrZUOmn8_ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add food info"
      ],
      "metadata": {
        "id": "F1et8EpmGEpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавим информацию о блюдах, которая собрана с платформы рецептов eda.ru. Данные уже были собраны неизвестными коллегами и размещены в репозитории https://github.com/Alenush/dish_id_sirius"
      ],
      "metadata": {
        "id": "ZRuZRicqWz0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('eda_all_recipes.csv')[['name', \"ingridient_keywords\"]]"
      ],
      "metadata": {
        "id": "JU1DAsdOExjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В датасете содержится 40122 уникальных названий блюд и 39961 уникальных наборов ингредиентов."
      ],
      "metadata": {
        "id": "L_hcwLSkXlaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "asmbprF5XeTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "id": "mjSp0cQpGK_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вытащим названия блюд, слишком длинные обрежем (не очень обоснованно, но попробуем)"
      ],
      "metadata": {
        "id": "iU3RxU0KXyvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#food = list(data['name'])\n",
        "#food = [x for x in food if isinstance(x, str)]\n",
        "#food = [item if len(item.split()) < 4 else ' '.join(item.split()[:3]) for item in food]"
      ],
      "metadata": {
        "id": "y-ltxszAmAx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Соберем все ингредиенты, выбросим стоп слова и слишком короткие названия(это не продукты, а граммы и прочее), лемматизируем"
      ],
      "metadata": {
        "id": "LYNL9mJlX9hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ingredients = list(data[\"ingridient_keywords\"].apply(lambda x: x[2:-2].split('\\', \\'')))\n",
        "ingredients = [item for sublist in ingredients for item in sublist if len(item)>2]\n",
        "ingredients = list(set(ingredients) - set(stopwords.words(\"russian\")))\n",
        "ingredients = [item for item in ingredients if morph.parse(item)[0].tag.POS == 'NOUN']\n",
        "ingredients_norm = [tuple(normalize(str(m))) for m in ingredients]\n"
      ],
      "metadata": {
        "id": "gKfncIdESwm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ingredients_norm)"
      ],
      "metadata": {
        "id": "8E-MF3kPSgRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сами названия блюд было решено не добавлять --- это не улучшало качество. Есть предположение, что в отзывах люди не пишут такие подробные названия блюд"
      ],
      "metadata": {
        "id": "I0ESMXOVYUPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#additional = ingredients + food\n",
        "#additional = [tuple(normalize(str(m))) for m in additional]"
      ],
      "metadata": {
        "id": "HgaNvLp2n4R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В первую очередь ищем аспекты на основе частотного подхода из бейзлайна. После пробуем найти на основе наших данных об ингредиентах. Возникает, конечно, вопрос, какую тональность приписывать аспектам, обнаруженным в списке ингредиентов."
      ],
      "metadata": {
        "id": "6u5KT4J8Yi6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_aspects[train_aspects.category == 'Food']['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "Nl9asD4uZWhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В тренировочной выборке люди чаще всего писали о еде положительно. Окей, попробуем каждому упоминанию еды не из тренировочной выборки присвоить положительную тональность."
      ],
      "metadata": {
        "id": "uF4G1MjLZ7_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def label_texts(text, mentions, sentiments, additional, max_len=5):\n",
        "    tokenized = [word for sent in nlp(text).sentences for word in sent.words]\n",
        "    text_end = len(tokenized)\n",
        "    for i, token in enumerate(tokenized):\n",
        "        for l in reversed(range(max_len)):\n",
        "            if i + l > text_end:\n",
        "                continue\n",
        "            span = tokenized[i:i + l]\n",
        "            key = tuple([t.lemma for t in span])\n",
        "            if key in mentions:\n",
        "                start, end = span[0].start_char, span[-1].end_char\n",
        "                yield mentions[key], text[start:end], start, end, sentiments[key]\n",
        "                break\n",
        "            elif key in additional:\n",
        "                start, end = span[0].start_char, span[-1].end_char\n",
        "                yield 'Food', text[start:end], start, end, 'positive'\n",
        "                break"
      ],
      "metadata": {
        "id": "z86L4AgbxU-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('dev_pred_aspects.txt', 'w') as f:\n",
        "    for text, idx in zip(dev_texts['text'], dev_texts['text_id']):\n",
        "        for asp in label_texts(text, best_mention_cat, best_mention_sentiment, ingredients_norm):\n",
        "            print(idx, *asp, sep=\"\\t\", file=f)\n"
      ],
      "metadata": {
        "id": "Skia6rZLYlzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation 1 & 2"
      ],
      "metadata": {
        "id": "FHL3eNryhMlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/named-entity/hse-nlp/master/4th_year/Project/dev_aspects.txt\n",
        "gold_test_path = \"dev_aspects.txt\"\n",
        "pred_test_path = \"dev_pred_aspects.txt\""
      ],
      "metadata": {
        "id": "Ix0WeKpZfDu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_aspect_cats = {}\n",
        "with open(gold_test_path) as fg:\n",
        "    for line in fg:\n",
        "        line = line.rstrip('\\r\\n').split('\\t')\n",
        "        if line[0] not in gold_aspect_cats:\n",
        "            gold_aspect_cats[line[0]] = {\"starts\":[], \"ends\":[], \"cats\":[], \"sents\":[]}\n",
        "        gold_aspect_cats[line[0]][\"starts\"].append(int(line[3]))\n",
        "        gold_aspect_cats[line[0]][\"ends\"].append(int(line[4]))\n",
        "        gold_aspect_cats[line[0]][\"cats\"].append(line[1])\n",
        "        gold_aspect_cats[line[0]][\"sents\"].append(line[5])\n",
        "full_match, partial_match, full_cat_match, partial_cat_match = 0, 0, 0, 0\n",
        "total = 0\n",
        "fully_matched_pairs = []\n",
        "partially_matched_pairs = []\n",
        "with open(pred_test_path) as fp:\n",
        "    for line in fp:    \n",
        "        total += 1\n",
        "        line = line.rstrip('\\r\\n').split('\\t')\n",
        "        start, end = int(line[3]), int(line[4])\n",
        "        category = line[1]\n",
        "        doc_gold_aspect_cats = gold_aspect_cats[line[0]]\n",
        "        if start in doc_gold_aspect_cats[\"starts\"]:\n",
        "            i = doc_gold_aspect_cats[\"starts\"].index(start)\n",
        "            if doc_gold_aspect_cats[\"ends\"][i] == end:\n",
        "                full_match += 1\n",
        "                if doc_gold_aspect_cats[\"cats\"][i] == category:\n",
        "                    full_cat_match += 1\n",
        "                else:\n",
        "                    partial_cat_match += 1\n",
        "                fully_matched_pairs.append(\n",
        "                    (\n",
        "                        [\n",
        "                            doc_gold_aspect_cats[\"starts\"][i], \n",
        "                            doc_gold_aspect_cats[\"ends\"][i], \n",
        "                            doc_gold_aspect_cats[\"cats\"][i],\n",
        "                            doc_gold_aspect_cats[\"sents\"][i]\n",
        "                        ],\n",
        "                        line\n",
        "                    )\n",
        "                )\n",
        "                continue\n",
        "        for s_pos in doc_gold_aspect_cats[\"starts\"]:\n",
        "            if start <= s_pos:\n",
        "                i = doc_gold_aspect_cats[\"starts\"].index(s_pos)\n",
        "                if doc_gold_aspect_cats[\"ends\"][i] == end:\n",
        "                    partial_match += 1\n",
        "                    partially_matched_pairs.append(\n",
        "                        (\n",
        "                            [\n",
        "                                doc_gold_aspect_cats[\"starts\"][i], \n",
        "                                doc_gold_aspect_cats[\"ends\"][i], \n",
        "                                doc_gold_aspect_cats[\"cats\"][i],\n",
        "                                doc_gold_aspect_cats[\"sents\"][i]\n",
        "                            ],\n",
        "                            line\n",
        "                        )\n",
        "                    )\n",
        "                    if doc_gold_aspect_cats[\"cats\"][i] == category:\n",
        "                        partial_cat_match += 1\n",
        "                    continue\n",
        "                matched = False\n",
        "                for e_pos in doc_gold_aspect_cats[\"ends\"][i:]:\n",
        "                    if s_pos <= end <= e_pos:\n",
        "                        partial_match += 1\n",
        "                        partially_matched_pairs.append(\n",
        "                            (\n",
        "                                [\n",
        "                                    doc_gold_aspect_cats[\"starts\"][i], \n",
        "                                    doc_gold_aspect_cats[\"ends\"][i], \n",
        "                                    doc_gold_aspect_cats[\"cats\"][i],\n",
        "                                    doc_gold_aspect_cats[\"sents\"][i]\n",
        "                                ],\n",
        "                                line\n",
        "                            )\n",
        "                        )\n",
        "                        if doc_gold_aspect_cats[\"cats\"][i] == category:\n",
        "                            partial_cat_match += 1\n",
        "                        matched = True\n",
        "                        break\n",
        "                if matched:\n",
        "                    break\n",
        "            if start > s_pos:\n",
        "                i = doc_gold_aspect_cats[\"starts\"].index(s_pos)\n",
        "                if start < doc_gold_aspect_cats[\"ends\"][i] <= end:\n",
        "                    partial_match += 1\n",
        "                    partially_matched_pairs.append(\n",
        "                        (\n",
        "                            [\n",
        "                                doc_gold_aspect_cats[\"starts\"][i], \n",
        "                                doc_gold_aspect_cats[\"ends\"][i], \n",
        "                                doc_gold_aspect_cats[\"cats\"][i],\n",
        "                                doc_gold_aspect_cats[\"sents\"][i]\n",
        "                            ],\n",
        "                            line\n",
        "                        )\n",
        "                    )\n",
        "                    if doc_gold_aspect_cats[\"cats\"][i] == category:\n",
        "                        partial_cat_match += 1\n",
        "                    break\n",
        "gold_size = sum([len(gold_aspect_cats[x][\"cats\"]) for x in gold_aspect_cats])\n",
        "print(f\"\"\"\n",
        "Full match precision: {full_match / total}\n",
        "Full match recall: {full_match / gold_size}\n",
        "Partial match ratio in pred: {(full_match + partial_match)  / total}\n",
        "Full category accuracy: {full_cat_match / total}\n",
        "Partial category accuracy: {(full_cat_match + partial_cat_match) / total}\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "GnhEYs9bfDsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_accuracy(matches):\n",
        "    matched_sentiment = 0.\n",
        "    for pair in matches:\n",
        "        *_, gold_s = pair[0]\n",
        "        *_, pred_s = pair[1]\n",
        "        if gold_s == pred_s:\n",
        "            matched_sentiment += 1\n",
        "    print(f\"Mention sentiment accuracy: {matched_sentiment / len(matches)}\")\n",
        "\n",
        "sentiment_accuracy(fully_matched_pairs)\n",
        "\n",
        "\n",
        "sentiment_accuracy(partially_matched_pairs)"
      ],
      "metadata": {
        "id": "_2QE3Tf8fDpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3"
      ],
      "metadata": {
        "id": "85VcYf_2p6Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORIES = ['Whole', 'Interior', 'Service', 'Food', 'Price']\n",
        "def get_full_sentiment(text, mentions, sentiment, max_len=5):\n",
        "    asp_counter = defaultdict(Counter)\n",
        "    for asp in label_texts(text, best_mention_cat, best_mention_sentiment, ingredients_norm, max_len):\n",
        "        category, *_, sentiment = asp\n",
        "        asp_counter[category][sentiment] += 1\n",
        "    for c in CATEGORIES:\n",
        "        if not asp_counter[c]:\n",
        "            s = 'absence'\n",
        "        elif len(asp_counter[c]) == 1:\n",
        "            s = asp_counter[c].most_common(1)[0][0]\n",
        "        else:\n",
        "            s = 'both'\n",
        "        yield c, s\n",
        "\n",
        "with open('dev_pred_cats.txt', 'w') as f:\n",
        "    for text, idx in zip(dev_texts['text'], dev_texts['text_id']):\n",
        "        for c, s in get_full_sentiment(text, best_mention_cat, best_mention_sentiment):\n",
        "            print(idx, c, s, sep=\"\\t\", file=f)"
      ],
      "metadata": {
        "id": "Mr8cfuV3flZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/named-entity/hse-nlp/master/4th_year/Project/dev_cats.txt\n",
        "gold_test_cats_path = \"dev_cats.txt\"\n",
        "pred_test_cats_path = \"dev_pred_cats.txt\"\n",
        "with open(gold_test_cats_path) as gc, open(pred_test_cats_path) as pc:\n",
        "    gold_labels = set(gc.readlines())\n",
        "    pred_labels = set(pc.readlines())\n",
        "    print(\n",
        "        \"Overall sentiment accuracy:\",\n",
        "        len(gold_labels & pred_labels) / len(gold_labels)\n",
        "    )"
      ],
      "metadata": {
        "id": "ZPGFKkgZNHx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A_6oPzK5TWJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}